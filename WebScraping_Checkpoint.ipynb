{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef908f1-cfeb-46d7-891b-06254d744062",
   "metadata": {},
   "source": [
    "The objective is to automate the extraction of HTML content, article titles, text, and internal links from Wikipedia pages into a consolidated function that accepts any Wikipedia URL for efficient data retrieval and processing.\n",
    "\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a Python script to automate data extraction from Wikipedia pages. The script will retrieve HTML content, extract article titles and text, collect internal links, and consolidate these tasks into one function that accepts a Wikipedia URL. This will be tested on a specific Wikipedia page to validate functionality.\n",
    "\n",
    "1) Write a function to Get and parse html content from a Wikipedia page\n",
    "\n",
    "2) Write a function to Extract article title\n",
    "\n",
    "3) Write a function to Extract article text for each paragraph with their respective\n",
    "\n",
    "headings. Map those headings to their respective paragraphs in the dictionary.\n",
    "\n",
    "4) Write a function to collect every link that redirects to another Wikipedia page\n",
    "\n",
    "5) Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
    "\n",
    "6) Test the last function on a Wikipedia page of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eff22a1-4efc-429d-9e7f-9961195d6103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Web scraping\n",
      "\n",
      "Content Preview:\n",
      "\n",
      "## Introduction:\n",
      "...\n",
      "\n",
      "Total internal links found: 133\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 1. Get and parse HTML content from a Wikipedia page\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "\n",
    "# 2. Extract article title\n",
    "def extract_title(soup):\n",
    "    return soup.find(\"h1\", {\"id\": \"firstHeading\"}).text.strip()\n",
    "\n",
    "# 3. Extract article text with headings mapped to paragraphs\n",
    "def extract_headings_and_paragraphs(soup):\n",
    "    content_div = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "    sections = {}\n",
    "    current_heading = \"Introduction\"\n",
    "    sections[current_heading] = []\n",
    "\n",
    "    for element in content_div.find_all(['h2', 'h3', 'p'], recursive=False):\n",
    "        if element.name in ['h2', 'h3']:\n",
    "            span = element.find(\"span\", class_=\"mw-headline\")\n",
    "            if span:\n",
    "                current_heading = span.text.strip()\n",
    "                sections[current_heading] = []\n",
    "        elif element.name == 'p':\n",
    "            paragraph = element.get_text(strip=True)\n",
    "            if paragraph:\n",
    "                sections[current_heading].append(paragraph)\n",
    "\n",
    "    # Combine list of paragraphs into single strings per heading\n",
    "    return {heading: \"\\n\".join(paragraphs) for heading, paragraphs in sections.items()}\n",
    "\n",
    "# 4. Collect every link that redirects to another Wikipedia page\n",
    "def extract_internal_links(soup):\n",
    "    links = set()\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"/wiki/\") and not any(prefix in href for prefix in [\":\", \"#\"]):\n",
    "            full_url = urljoin(\"https://en.wikipedia.org\", href)\n",
    "            links.add(full_url)\n",
    "    return list(links)\n",
    "\n",
    "# 5. Wrap all into one function\n",
    "def extract_wikipedia_data(url):\n",
    "    soup = fetch_html(url)\n",
    "    title = extract_title(soup)\n",
    "    content = extract_headings_and_paragraphs(soup)\n",
    "    internal_links = extract_internal_links(soup)\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"internal_links\": internal_links\n",
    "    }\n",
    "\n",
    "# 6. Test the function\n",
    "if __name__ == \"__main__\":\n",
    "    test_url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "    data = extract_wikipedia_data(test_url)\n",
    "    \n",
    "    print(\"Title:\", data[\"title\"])\n",
    "    print(\"\\nContent Preview:\")\n",
    "    for heading, paragraph in list(data[\"content\"].items())[:3]:\n",
    "        print(f\"\\n## {heading}:\\n{paragraph[:300]}...\")  # Preview first 300 chars\n",
    "    print(f\"\\nTotal internal links found: {len(data['internal_links'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1105f-fb5e-4b90-a599-587acb0f0252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
